{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-18T16:02:14.842166Z",
     "iopub.status.busy": "2024-12-18T16:02:14.841766Z",
     "iopub.status.idle": "2024-12-18T16:02:33.149149Z",
     "shell.execute_reply": "2024-12-18T16:02:33.148201Z",
     "shell.execute_reply.started": "2024-12-18T16:02:14.842131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.0.0 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:28.779807Z",
     "iopub.status.busy": "2024-12-18T16:01:28.779450Z",
     "iopub.status.idle": "2024-12-18T16:01:36.273868Z",
     "shell.execute_reply": "2024-12-18T16:01:36.272982Z",
     "shell.execute_reply.started": "2024-12-18T16:01:28.779770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6739c79e38b415a9a6f9c821b60f7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3e223a190e4d40bb97cec9d78a1951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NusaX-MT.py:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for indonlp/NusaX-MT contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/indonlp/NusaX-MT.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c6702aba59422fa7c1f7c92a426205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/935k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf73bcc39d744f1181ec598b6533ec55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a744ce1a1a494c3ab05b515fe44960ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/751k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7326de5896ac427891ea6371a183d1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0906f64b6b4e95b4ac4b6a121ec272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd64880fead3491d8c3dba04990ac160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# text_1 -> ind, text_2 -> mad\n",
    "mdr = load_dataset(\"indonlp/NusaX-MT\",name=\"ind-mad\") \n",
    "mdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:36.275968Z",
     "iopub.status.busy": "2024-12-18T16:01:36.275595Z",
     "iopub.status.idle": "2024-12-18T16:01:36.280480Z",
     "shell.execute_reply": "2024-12-18T16:01:36.279573Z",
     "shell.execute_reply.started": "2024-12-18T16:01:36.275931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = mdr[\"train\"]\n",
    "val = mdr[\"validation\"]\n",
    "test = mdr[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:36.281648Z",
     "iopub.status.busy": "2024-12-18T16:01:36.281396Z",
     "iopub.status.idle": "2024-12-18T16:01:39.662585Z",
     "shell.execute_reply": "2024-12-18T16:01:39.661767Z",
     "shell.execute_reply.started": "2024-12-18T16:01:36.281622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367a43696ee54d2a9e3683aec689ed25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9030d53893904292957af31cb8411297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.utils import move_cache\n",
    "move_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:39.665527Z",
     "iopub.status.busy": "2024-12-18T16:01:39.664707Z",
     "iopub.status.idle": "2024-12-18T16:01:42.712142Z",
     "shell.execute_reply": "2024-12-18T16:01:42.711373Z",
     "shell.execute_reply.started": "2024-12-18T16:01:39.665483Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6521fc556dd74b929a808a48b95ba235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/330 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9825c92b60734537ab67fe9b9e8a0f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003741dcda7e4405bf0585734e2d570a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2187b1b525784f9c8f24b71e4e15085f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/indonlp/cendol-mt5-small-inst\n",
    "checkpoint = \"indonlp/cendol-mt5-small-inst\" # GANTI MODELNYA\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:42.713529Z",
     "iopub.status.busy": "2024-12-18T16:01:42.713114Z",
     "iopub.status.idle": "2024-12-18T16:01:42.719568Z",
     "shell.execute_reply": "2024-12-18T16:01:42.718655Z",
     "shell.execute_reply.started": "2024-12-18T16:01:42.713501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prefix = \"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: \" # nanti pas inference, perlu di append prefix nya!\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Menambahkan prefix dan melakukan lowercasing pada input\n",
    "    inputs = [prefix + example.lower() for example in examples[\"text_2\"]]  # optional: .lower() jika perlu\n",
    "    targets = [example.lower() for example in examples[\"text_1\"]]  # optional: .lower() jika perlu\n",
    "    \n",
    "    # Tokenisasi\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:46.341363Z",
     "iopub.status.busy": "2024-12-18T16:01:46.340475Z",
     "iopub.status.idle": "2024-12-18T16:01:46.575223Z",
     "shell.execute_reply": "2024-12-18T16:01:46.574233Z",
     "shell.execute_reply.started": "2024-12-18T16:01:46.341325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82b11264e154eebabbc9ba7f6f88bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19954ef4a474d16b50eafaf16671122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_val = val.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:01:49.045253Z",
     "iopub.status.busy": "2024-12-18T16:01:49.044423Z",
     "iopub.status.idle": "2024-12-18T16:02:00.085068Z",
     "shell.execute_reply": "2024-12-18T16:02:00.084350Z",
     "shell.execute_reply.started": "2024-12-18T16:01:49.045212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:02:33.152458Z",
     "iopub.status.busy": "2024-12-18T16:02:33.151568Z",
     "iopub.status.idle": "2024-12-18T16:02:35.295164Z",
     "shell.execute_reply": "2024-12-18T16:02:35.294082Z",
     "shell.execute_reply.started": "2024-12-18T16:02:33.152409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e397d4419942b8a51b2a6954560f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process texts\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    # Calculate average generation length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = round(np.mean(prediction_lens), 4)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:02:35.296742Z",
     "iopub.status.busy": "2024-12-18T16:02:35.296232Z",
     "iopub.status.idle": "2024-12-18T16:02:48.000620Z",
     "shell.execute_reply": "2024-12-18T16:02:47.999831Z",
     "shell.execute_reply.started": "2024-12-18T16:02:35.296711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71921ffb1dd54f87829813f3a35ec808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def28b29fd1a4eaa88c9b217eb5372ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f255678de84274a31559f44fb20c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:02:52.731536Z",
     "iopub.status.busy": "2024-12-18T16:02:52.731172Z",
     "iopub.status.idle": "2024-12-18T16:12:35.310371Z",
     "shell.execute_reply": "2024-12-18T16:12:35.309483Z",
     "shell.execute_reply.started": "2024-12-18T16:02:52.731502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3685775069.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241218_160305-8bagqjif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/najladhia259-airlangga/huggingface/runs/8bagqjif' target=\"_blank\">mad-indo_cendol-mt5-small-inst_25</a></strong> to <a href='https://wandb.ai/najladhia259-airlangga/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/najladhia259-airlangga/huggingface' target=\"_blank\">https://wandb.ai/najladhia259-airlangga/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/najladhia259-airlangga/huggingface/runs/8bagqjif' target=\"_blank\">https://wandb.ai/najladhia259-airlangga/huggingface/runs/8bagqjif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [896/896 09:27, Epoch 28/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.386802</td>\n",
       "      <td>32.723510</td>\n",
       "      <td>34.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.325022</td>\n",
       "      <td>35.067202</td>\n",
       "      <td>34.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.307034</td>\n",
       "      <td>36.186774</td>\n",
       "      <td>34.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.296602</td>\n",
       "      <td>36.157660</td>\n",
       "      <td>34.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.311493</td>\n",
       "      <td>37.060024</td>\n",
       "      <td>34.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.304290</td>\n",
       "      <td>37.769231</td>\n",
       "      <td>34.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.328349</td>\n",
       "      <td>37.110327</td>\n",
       "      <td>34.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.324803</td>\n",
       "      <td>38.478755</td>\n",
       "      <td>34.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.339374</td>\n",
       "      <td>37.632639</td>\n",
       "      <td>34.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.349649</td>\n",
       "      <td>38.584406</td>\n",
       "      <td>34.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.323225</td>\n",
       "      <td>38.410438</td>\n",
       "      <td>34.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.277686</td>\n",
       "      <td>37.741025</td>\n",
       "      <td>34.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.277693</td>\n",
       "      <td>37.978636</td>\n",
       "      <td>34.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279973</td>\n",
       "      <td>38.304702</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.279985</td>\n",
       "      <td>38.416518</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.280014</td>\n",
       "      <td>38.432889</td>\n",
       "      <td>34.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279897</td>\n",
       "      <td>38.414804</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279791</td>\n",
       "      <td>38.304702</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279791</td>\n",
       "      <td>38.432889</td>\n",
       "      <td>34.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279759</td>\n",
       "      <td>38.416518</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279787</td>\n",
       "      <td>38.416518</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279664</td>\n",
       "      <td>38.288365</td>\n",
       "      <td>34.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279641</td>\n",
       "      <td>38.288365</td>\n",
       "      <td>34.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279597</td>\n",
       "      <td>38.288365</td>\n",
       "      <td>34.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279551</td>\n",
       "      <td>38.288365</td>\n",
       "      <td>34.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279622</td>\n",
       "      <td>38.416518</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279596</td>\n",
       "      <td>38.416518</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.279513</td>\n",
       "      <td>38.416518</td>\n",
       "      <td>34.620000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=896, training_loss=0.7953644139426095, metrics={'train_runtime': 580.539, 'train_samples_per_second': 24.116, 'train_steps_per_second': 1.543, 'total_flos': 1783869269606400.0, 'train_loss': 0.7953644139426095, 'epoch': 28.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mad-indo_cendol-mt5-small-inst_25\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=28,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    generation_max_length=50\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:13:08.588043Z",
     "iopub.status.busy": "2024-12-18T16:13:08.587770Z",
     "iopub.status.idle": "2024-12-18T16:13:08.619066Z",
     "shell.execute_reply": "2024-12-18T16:13:08.618250Z",
     "shell.execute_reply.started": "2024-12-18T16:13:08.588016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_test = test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:12:38.079516Z",
     "iopub.status.busy": "2024-12-18T16:12:38.078921Z",
     "iopub.status.idle": "2024-12-18T16:13:08.586416Z",
     "shell.execute_reply": "2024-12-18T16:13:08.585543Z",
     "shell.execute_reply.started": "2024-12-18T16:12:38.079479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU score: 36.53391902933586\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(\"Test BLEU score:\", test_results[\"eval_bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:13:08.620405Z",
     "iopub.status.busy": "2024-12-18T16:13:08.620098Z",
     "iopub.status.idle": "2024-12-18T16:13:08.633092Z",
     "shell.execute_reply": "2024-12-18T16:13:08.632293Z",
     "shell.execute_reply.started": "2024-12-18T16:13:08.620378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1646003723144531,\n",
       " 'eval_bleu': 36.53391902933586,\n",
       " 'eval_gen_len': 34.3725,\n",
       " 'eval_runtime': 27.9181,\n",
       " 'eval_samples_per_second': 14.328,\n",
       " 'eval_steps_per_second': 0.895,\n",
       " 'epoch': 28.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE AND LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BUAT NGESAVE KE LOCAL!!!\n",
    "local_dir = \"cendol-mt5-small-inst\"\n",
    "\n",
    "model.save_pretrained(local_dir)\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "\n",
    "# NANTI CARA NGE LOAD NYA GINI!!\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "\n",
    "def translate_and_compare_samples(test_data, num_samples=5):\n",
    "    samples = random.sample(range(len(test_data['text_1'])), num_samples)\n",
    "    \n",
    "    for idx in samples:\n",
    "        source_text = test_data['text_1'][idx]\n",
    "        target_text = test_data['text_2'][idx]\n",
    "        text = f\"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: {source_text}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Sampel {idx + 1}\")\n",
    "        print(\"Source Text (Indonesia):\", source_text)\n",
    "        print(\"Translated Text (Madura):\", translated_text)\n",
    "        print(\"Target Text (Expected Madura):\", target_text)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "translate_and_compare_samples(test, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "\n",
    "text = \"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: gimana kabarmu?\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Inisialisasi model dan tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "\n",
    "# Teks masukan dalam bahasa Madura\n",
    "text = \"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: Semmak bik hotel engkok nginep, pera' ejeleni ajelen soko, ediye bennyak sarah pelean kakananna, kenengngan se leber, ben masenneng\"\n",
    "\n",
    "# Tokenisasi input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generasi terjemahan\n",
    "outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Dekode hasil terjemahan\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import pandas as pd\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# import sacrebleu\n",
    "\n",
    "# # Inisialisasi model dan tokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "\n",
    "# # Fungsi untuk menerjemahkan dan membandingkan hasil dengan skor BLEU\n",
    "# def translate_and_compare_samples(test_data, num_samples=100):\n",
    "#     # Ambil beberapa sampel acak dari data test\n",
    "#     samples = random.sample(range(len(test_data['text_1'])), num_samples)\n",
    "\n",
    "#     # List untuk menyimpan hasil yang akan dikonversi menjadi tabel\n",
    "#     results = []\n",
    "\n",
    "#     for idx in samples:\n",
    "#         source_text = test_data['text_1'][idx]\n",
    "#         target_text = test_data['text_2'][idx]\n",
    "#         text = f\"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: {source_text}\"\n",
    "        \n",
    "#         # Tokenisasi dan terjemahan\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "#         outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "        \n",
    "#         # Dekode hasil terjemahan\n",
    "#         translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "#         # Hitung skor BLEU\n",
    "#         bleu = sacrebleu.corpus_bleu([translated_text], [[target_text]]).score\n",
    "        \n",
    "#         # Tambahkan hasil ke list\n",
    "#         results.append({\n",
    "#             \"Source Text (Indonesia)\": source_text,\n",
    "#             \"Translated Text (Madura)\": translated_text,\n",
    "#             \"Target Text (Expected Madura)\": target_text,\n",
    "#             \"BLEU Score\": bleu\n",
    "#         })\n",
    "\n",
    "#     # Konversi hasil ke dalam DataFrame pandas\n",
    "#     df_results = pd.DataFrame(results)\n",
    "#     return df_results\n",
    "\n",
    "# # Panggil fungsi dan tampilkan hasil dalam bentuk tabel\n",
    "# df_results = translate_and_compare_samples(test, num_samples=100)\n",
    "# df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_results.to_csv(\"translation_results_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
