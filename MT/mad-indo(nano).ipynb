{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n!pip install sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-18T16:45:26.406804Z","iopub.execute_input":"2024-12-18T16:45:26.407054Z","iopub.status.idle":"2024-12-18T16:45:44.785605Z","shell.execute_reply.started":"2024-12-18T16:45:26.407028Z","shell.execute_reply":"2024-12-18T16:45:44.784703Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.0.0 sacrebleu-2.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Load","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# text_1 -> ind, text_2 -> mad\nmdr = load_dataset(\"indonlp/NusaX-MT\",name=\"ind-mad\") \nmdr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:45:44.787282Z","iopub.execute_input":"2024-12-18T16:45:44.787572Z","iopub.status.idle":"2024-12-18T16:45:58.492893Z","shell.execute_reply.started":"2024-12-18T16:45:44.787545Z","shell.execute_reply":"2024-12-18T16:45:58.491939Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.64k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235f02528d864529bce49bb057380227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"NusaX-MT.py:   0%|          | 0.00/5.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e5f11956f544b4be3f4c660589b8d8"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for indonlp/NusaX-MT contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/indonlp/NusaX-MT.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/935k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8521e013771a43bdbf0f26c052a11ea3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22458ca9089f4c70af8a4e189fac48f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/751k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"987e39e71c814b969847fb840707cb07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d28dc7277463405591d8712287ec27f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cec2daa43e048a8ae58e6fd7482ef04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a392d7ed2aa44a33b9668b3dd0adbf46"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n        num_rows: 500\n    })\n    validation: Dataset({\n        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n        num_rows: 100\n    })\n    test: Dataset({\n        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n        num_rows: 400\n    })\n})"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"train = mdr[\"train\"]\nval = mdr[\"validation\"]\ntest = mdr[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:45:58.493862Z","iopub.execute_input":"2024-12-18T16:45:58.494127Z","iopub.status.idle":"2024-12-18T16:45:58.498221Z","shell.execute_reply.started":"2024-12-18T16:45:58.494101Z","shell.execute_reply":"2024-12-18T16:45:58.497282Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers.utils import move_cache\nmove_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:45:58.500073Z","iopub.execute_input":"2024-12-18T16:45:58.500341Z","iopub.status.idle":"2024-12-18T16:46:01.643070Z","shell.execute_reply.started":"2024-12-18T16:45:58.500315Z","shell.execute_reply":"2024-12-18T16:46:01.642281Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6991487f7e54915a6649b9264930cd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4bfb73ac0b6408d8e0f23ee0a5b506a"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# https://huggingface.co/indonlp/cendol-mt5-small-inst\ncheckpoint = \"LazarusNLP/IndoNanoT5-base\" # GANTI MODELNYA\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:01.644227Z","iopub.execute_input":"2024-12-18T16:46:01.644753Z","iopub.status.idle":"2024-12-18T16:46:03.477091Z","shell.execute_reply.started":"2024-12-18T16:46:01.644711Z","shell.execute_reply":"2024-12-18T16:46:03.476332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b16150a81847738634888ac5cf52b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292b5303d2ea40a6877f6960ac11e762"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7322b66f514f2f9c8c100446c97f75"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"prefix = \"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: \" # nanti pas inference, perlu di append prefix nya!\n\ndef preprocess_function(examples):\n    # Menambahkan prefix dan melakukan lowercasing pada input\n    inputs = [prefix + example.lower() for example in examples[\"text_2\"]]  # optional: .lower() jika perlu\n    targets = [example.lower() for example in examples[\"text_1\"]]  # optional: .lower() jika perlu\n    \n    # Tokenisasi\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    \n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:03.478041Z","iopub.execute_input":"2024-12-18T16:46:03.478460Z","iopub.status.idle":"2024-12-18T16:46:03.483367Z","shell.execute_reply.started":"2024-12-18T16:46:03.478432Z","shell.execute_reply":"2024-12-18T16:46:03.482561Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenized_train = train.map(preprocess_function, batched=True)\ntokenized_val = val.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:03.484453Z","iopub.execute_input":"2024-12-18T16:46:03.484798Z","iopub.status.idle":"2024-12-18T16:46:03.662564Z","shell.execute_reply.started":"2024-12-18T16:46:03.484761Z","shell.execute_reply":"2024-12-18T16:46:03.661701Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa082af937748e7976608fc366f8571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d45dbe98db4f51a52cbea861b67110"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:03.663849Z","iopub.execute_input":"2024-12-18T16:46:03.664314Z","iopub.status.idle":"2024-12-18T16:46:14.650432Z","shell.execute_reply.started":"2024-12-18T16:46:03.664273Z","shell.execute_reply":"2024-12-18T16:46:14.649626Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nmetric = evaluate.load(\"sacrebleu\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Post-process texts\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # Compute BLEU score\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    # Calculate average generation length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = round(np.mean(prediction_lens), 4)\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:14.651454Z","iopub.execute_input":"2024-12-18T16:46:14.651953Z","iopub.status.idle":"2024-12-18T16:46:16.886416Z","shell.execute_reply.started":"2024-12-18T16:46:14.651925Z","shell.execute_reply":"2024-12-18T16:46:16.885487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1931cad68ba4c8d8120270d9906f648"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:16.888559Z","iopub.execute_input":"2024-12-18T16:46:16.889065Z","iopub.status.idle":"2024-12-18T16:46:42.894822Z","shell.execute_reply.started":"2024-12-18T16:46:16.889036Z","shell.execute_reply":"2024-12-18T16:46:42.894029Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237655b0347945f68a173113ee10a6a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64247b2d613a4df0b36b2207bfe2054c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ee6e0a7edfb4145a62a6f0e44cf6192"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"mad-indo_IndoNano-T5_25\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=28,\n    predict_with_generate=True,\n    fp16=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n#8e273a83f932e54c56230b4e303a2f335701ad7b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:46:42.896417Z","iopub.execute_input":"2024-12-18T16:46:42.896776Z","iopub.status.idle":"2024-12-18T16:56:15.642957Z","shell.execute_reply.started":"2024-12-18T16:46:42.896736Z","shell.execute_reply":"2024-12-18T16:56:15.642235Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/239706790.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241218_164656-c27d3c5u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/najladhia259-airlangga/huggingface/runs/c27d3c5u' target=\"_blank\">mad-indo_IndoNano-T5_25</a></strong> to <a href='https://wandb.ai/najladhia259-airlangga/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/najladhia259-airlangga/huggingface' target=\"_blank\">https://wandb.ai/najladhia259-airlangga/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/najladhia259-airlangga/huggingface/runs/c27d3c5u' target=\"_blank\">https://wandb.ai/najladhia259-airlangga/huggingface/runs/c27d3c5u</a>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 09:16, Epoch 28/28]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>4.269657</td>\n      <td>1.296358</td>\n      <td>15.560000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>3.874518</td>\n      <td>2.267369</td>\n      <td>16.590000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>3.483699</td>\n      <td>3.307245</td>\n      <td>17.020000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>3.327253</td>\n      <td>4.697349</td>\n      <td>16.330000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>3.136981</td>\n      <td>5.712170</td>\n      <td>16.690000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>3.019821</td>\n      <td>6.664392</td>\n      <td>17.360000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>2.964751</td>\n      <td>7.939129</td>\n      <td>17.400000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>2.924191</td>\n      <td>7.819902</td>\n      <td>17.420000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>2.888624</td>\n      <td>8.751884</td>\n      <td>17.510000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>2.869298</td>\n      <td>9.514506</td>\n      <td>17.570000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>2.864176</td>\n      <td>9.605973</td>\n      <td>17.320000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>2.840874</td>\n      <td>10.041883</td>\n      <td>17.560000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>No log</td>\n      <td>2.833124</td>\n      <td>10.662204</td>\n      <td>17.440000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>No log</td>\n      <td>2.875991</td>\n      <td>10.828020</td>\n      <td>17.080000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>No log</td>\n      <td>2.846680</td>\n      <td>10.930968</td>\n      <td>17.110000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.563600</td>\n      <td>2.843034</td>\n      <td>11.011223</td>\n      <td>17.230000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.563600</td>\n      <td>2.867249</td>\n      <td>11.222997</td>\n      <td>17.120000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.563600</td>\n      <td>2.889307</td>\n      <td>11.140933</td>\n      <td>17.140000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.563600</td>\n      <td>2.907683</td>\n      <td>11.632241</td>\n      <td>17.030000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.563600</td>\n      <td>2.906947</td>\n      <td>11.682652</td>\n      <td>17.230000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.563600</td>\n      <td>2.924379</td>\n      <td>11.864960</td>\n      <td>17.100000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.563600</td>\n      <td>2.934850</td>\n      <td>11.425107</td>\n      <td>17.140000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.563600</td>\n      <td>2.942873</td>\n      <td>11.616773</td>\n      <td>17.140000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.563600</td>\n      <td>2.957662</td>\n      <td>11.699040</td>\n      <td>17.110000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.563600</td>\n      <td>2.959355</td>\n      <td>11.403093</td>\n      <td>17.080000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.563600</td>\n      <td>2.961729</td>\n      <td>11.951791</td>\n      <td>17.140000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.563600</td>\n      <td>2.959582</td>\n      <td>11.910079</td>\n      <td>17.150000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.563600</td>\n      <td>2.961736</td>\n      <td>11.760633</td>\n      <td>17.120000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=896, training_loss=1.8986884866442, metrics={'train_runtime': 570.9449, 'train_samples_per_second': 24.521, 'train_steps_per_second': 1.569, 'total_flos': 2342212524048384.0, 'train_loss': 1.8986884866442, 'epoch': 28.0})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"tokenized_test = test.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:56:15.644020Z","iopub.execute_input":"2024-12-18T16:56:15.644309Z","iopub.status.idle":"2024-12-18T16:56:15.745531Z","shell.execute_reply.started":"2024-12-18T16:56:15.644282Z","shell.execute_reply":"2024-12-18T16:56:15.744694Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc506e785aee4cd6b21f379a71f14fcd"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"test_results = trainer.evaluate(tokenized_test)\nprint(\"Test BLEU score:\", test_results[\"eval_bleu\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:56:15.746569Z","iopub.execute_input":"2024-12-18T16:56:15.746829Z","iopub.status.idle":"2024-12-18T16:56:32.783861Z","shell.execute_reply.started":"2024-12-18T16:56:15.746803Z","shell.execute_reply":"2024-12-18T16:56:32.783095Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:16]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test BLEU score: 11.37020697130227\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:56:32.784839Z","iopub.execute_input":"2024-12-18T16:56:32.785104Z","iopub.status.idle":"2024-12-18T16:56:32.790827Z","shell.execute_reply.started":"2024-12-18T16:56:32.785077Z","shell.execute_reply":"2024-12-18T16:56:32.789964Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 2.789724111557007,\n 'eval_bleu': 11.37020697130227,\n 'eval_gen_len': 17.29,\n 'eval_runtime': 17.0276,\n 'eval_samples_per_second': 23.491,\n 'eval_steps_per_second': 1.468,\n 'epoch': 28.0}"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## SAVE AND LOAD","metadata":{}},{"cell_type":"code","source":"# BUAT NGESAVE KE LOCAL!!!\nlocal_dir = \"cendol-mt5-small-inst\"\n\nmodel.save_pretrained(local_dir)\ntokenizer.save_pretrained(local_dir)\n\n# NANTI CARA NGE LOAD NYA GINI!!\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n# tokenizer = AutoTokenizer.from_pretrained(local_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\ntokenizer = AutoTokenizer.from_pretrained(local_dir)\n\ndef translate_and_compare_samples(test_data, num_samples=5):\n    samples = random.sample(range(len(test_data['text_1'])), num_samples)\n    \n    for idx in samples:\n        source_text = test_data['text_1'][idx]\n        target_text = test_data['text_2'][idx]\n        text = f\"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: {source_text}\"\n        inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n        outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Sampel {idx + 1}\")\n        print(\"Source Text (Indonesia):\", source_text)\n        print(\"Translated Text (Madura):\", translated_text)\n        print(\"Target Text (Expected Madura):\", target_text)\n        print(\"=\" * 50)\n\ntranslate_and_compare_samples(test, num_samples=5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\ntokenizer = AutoTokenizer.from_pretrained(local_dir)\n\ntext = \"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: gimana kabarmu?\"\n\ninputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\noutputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\ntranslated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Translated text:\", translated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Inisialisasi model dan tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\ntokenizer = AutoTokenizer.from_pretrained(local_dir)\n\n# Teks masukan dalam bahasa Madura\ntext = \"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: Semmak bik hotel engkok nginep, pera' ejeleni ajelen soko, ediye bennyak sarah pelean kakananna, kenengngan se leber, ben masenneng\"\n\n# Tokenisasi input\ninputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n\n# Generasi terjemahan\noutputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n# Dekode hasil terjemahan\ntranslated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Translated text:\", translated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n# import pandas as pd\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# import sacrebleu\n\n# # Inisialisasi model dan tokenizer\n# model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n# tokenizer = AutoTokenizer.from_pretrained(local_dir)\n\n# # Fungsi untuk menerjemahkan dan membandingkan hasil dengan skor BLEU\n# def translate_and_compare_samples(test_data, num_samples=100):\n#     # Ambil beberapa sampel acak dari data test\n#     samples = random.sample(range(len(test_data['text_1'])), num_samples)\n\n#     # List untuk menyimpan hasil yang akan dikonversi menjadi tabel\n#     results = []\n\n#     for idx in samples:\n#         source_text = test_data['text_1'][idx]\n#         target_text = test_data['text_2'][idx]\n#         text = f\"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: {source_text}\"\n        \n#         # Tokenisasi dan terjemahan\n#         inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n#         outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n        \n#         # Dekode hasil terjemahan\n#         translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n#         # Hitung skor BLEU\n#         bleu = sacrebleu.corpus_bleu([translated_text], [[target_text]]).score\n        \n#         # Tambahkan hasil ke list\n#         results.append({\n#             \"Source Text (Indonesia)\": source_text,\n#             \"Translated Text (Madura)\": translated_text,\n#             \"Target Text (Expected Madura)\": target_text,\n#             \"BLEU Score\": bleu\n#         })\n\n#     # Konversi hasil ke dalam DataFrame pandas\n#     df_results = pd.DataFrame(results)\n#     return df_results\n\n# # Panggil fungsi dan tampilkan hasil dalam bentuk tabel\n# df_results = translate_and_compare_samples(test, num_samples=100)\n# df_results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_results.to_csv(\"translation_results_full.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}